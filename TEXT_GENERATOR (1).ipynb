{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import  stopwords\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"frankenstein.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(input):\n",
    "    input = input.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \"\".join(filtered)\n",
    "processed_inputs = tokenize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c,i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 220857\n",
      "Total vocab: 42\n"
     ]
    }
   ],
   "source": [
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "print(\"Total number of characters:\",input_len)\n",
    "print(\"Total vocab:\",vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 220757\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, input_len - seq_length, 1):\n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])\n",
    "    \n",
    "n_patterns = len(x_data)\n",
    "print(\"Total Patterns:\", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np_utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"model_weights_saved.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks=[checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.9295\n",
      "Epoch 00001: loss improved from inf to 2.92947, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 134s 155ms/step - loss: 2.9295\n",
      "Epoch 2/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.9087\n",
      "Epoch 00002: loss improved from 2.92947 to 2.90866, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 135s 156ms/step - loss: 2.9087\n",
      "Epoch 3/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.9033\n",
      "Epoch 00003: loss improved from 2.90866 to 2.90334, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 135s 156ms/step - loss: 2.9033\n",
      "Epoch 4/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.8785\n",
      "Epoch 00004: loss improved from 2.90334 to 2.87847, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 135s 156ms/step - loss: 2.8785\n",
      "Epoch 5/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.8502\n",
      "Epoch 00005: loss improved from 2.87847 to 2.85019, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 135s 156ms/step - loss: 2.8502\n",
      "Epoch 6/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.8230\n",
      "Epoch 00006: loss improved from 2.85019 to 2.82297, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 135s 156ms/step - loss: 2.8230\n",
      "Epoch 7/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.7845\n",
      "Epoch 00007: loss improved from 2.82297 to 2.78453, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 135s 156ms/step - loss: 2.7845\n",
      "Epoch 8/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.7312\n",
      "Epoch 00008: loss improved from 2.78453 to 2.73118, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 135s 156ms/step - loss: 2.7312\n",
      "Epoch 9/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.6758\n",
      "Epoch 00009: loss improved from 2.73118 to 2.67577, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 135s 156ms/step - loss: 2.6758\n",
      "Epoch 10/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.6259\n",
      "Epoch 00010: loss improved from 2.67577 to 2.62591, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 135s 156ms/step - loss: 2.6259\n",
      "Epoch 11/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.5776\n",
      "Epoch 00011: loss improved from 2.62591 to 2.57755, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 135s 156ms/step - loss: 2.5776\n",
      "Epoch 12/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.5359\n",
      "Epoch 00012: loss improved from 2.57755 to 2.53593, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 135s 156ms/step - loss: 2.5359\n",
      "Epoch 13/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.4977\n",
      "Epoch 00013: loss improved from 2.53593 to 2.49767, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 135s 156ms/step - loss: 2.4977\n",
      "Epoch 14/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.4601\n",
      "Epoch 00014: loss improved from 2.49767 to 2.46010, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 135s 156ms/step - loss: 2.4601\n",
      "Epoch 15/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.4275\n",
      "Epoch 00015: loss improved from 2.46010 to 2.42748, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 135s 156ms/step - loss: 2.4275\n",
      "Epoch 16/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.3957\n",
      "Epoch 00016: loss improved from 2.42748 to 2.39571, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 135s 157ms/step - loss: 2.3957\n",
      "Epoch 17/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.3680\n",
      "Epoch 00017: loss improved from 2.39571 to 2.36803, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 135s 157ms/step - loss: 2.3680\n",
      "Epoch 18/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.3429\n",
      "Epoch 00018: loss improved from 2.36803 to 2.34292, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 135s 156ms/step - loss: 2.3429\n",
      "Epoch 19/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.3170\n",
      "Epoch 00019: loss improved from 2.34292 to 2.31705, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 135s 156ms/step - loss: 2.3170\n",
      "Epoch 20/20\n",
      "863/863 [==============================] - ETA: 0s - loss: 2.2922\n",
      "Epoch 00020: loss improved from 2.31705 to 2.29215, saving model to model_weights_saved.hdf5\n",
      "863/863 [==============================] - 135s 156ms/step - loss: 2.2922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f66b8d6b0f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y, epochs=20, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"model_weights_saved.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_char=dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:\n",
      "\" proachcoldweatherbeginninglastwinterjustinereturnedusassurelovetenderlyclevergentleextremelyprettyme \"\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed:\")\n",
    "print(\"\\\"\", '' . join([num_to_char[value] for value in pattern]) , \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deedesertedsestedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedsertedserte"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1,len(pattern), 1))\n",
    "    x = x/float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
